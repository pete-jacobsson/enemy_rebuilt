{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce6b9914-1ce5-46cd-9625-a49a69f44690",
   "metadata": {},
   "source": [
    "# Physical Geography\n",
    "\n",
    "**NOTE: the river and sea setup below uses an older version of the data management structure.**\n",
    "\n",
    "Most of those items have been moved \"off camera\" to the Google docs, only retaining the core physical geography object under [wfrp/physical_layers/inputs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9809fcc4-4f08-4bee-84e6-c8b406d04925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from world_simulator import run_river_skeleton_pipeline, test_noise, generate_fractal_noise\n",
    "from world_simulator import GPUThermalEroder, CoastalTaper\n",
    "\n",
    "from world_simulator import HydrologyAnalyzer, validate_arrow_directions, plot_flow, CalculateFlowMagnitude, plot_river_hierarchy, assign_river_widths, plot_river_physics, save_hydro_network, validate_topology_continuity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from gdgtm import change_raster_res\n",
    "\n",
    "\n",
    "vector_src_dir = \"/home/pete/Documents/wfrp/source_vectors/\"\n",
    "raster_src_dir = \"/home/pete/Documents/wfrp/source_rasters/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3265c09c-05a8-4da5-963a-46bff78dd70e",
   "metadata": {},
   "source": [
    "## River and sea setup\n",
    "\n",
    "Here we take a georeferenced, trimmed corase altitude map, we load it, increase resolution, and then we smooth it.\n",
    "\n",
    "We use an erosion approach to overal smoothing and aim for Int16 data type - which gives us some serious mileage in the \"disk saving department\" - 100s of MBs.\n",
    "\n",
    "The exact maths are indicated in the class docstring for the GPUThermalEroder. For now it is relevant to note that we went for a short erosion (100 steps), slow erosion rate (0.05), and a high threshold of 2. What this means is that we preserve most of the high ground on the map, while introducing just a little smoothing around the slopes to remove the blockiness. Not perfect, but the step below will take care of doing perfect :)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd92dbe-bde7-4a5c-be77-40d0ae6f29c4",
   "metadata": {},
   "source": [
    "### Vectorizing the river net from QGIS polys\n",
    "\n",
    "The goal here is to take a bunch of QGIS polygons and change them into an actual net of lines for downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d8d2a9-280e-4b73-86db-a17041ab06eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     # Example configuration\n",
    "#     IN_FILE = os.path.join(vector_src_dir, \"wfrp_empire_rivers_poly.gpkg\")\n",
    "#     OUT_FILE = os.path.join(vector_src_dir, \"wfrp_empire_rivers_line.gpkg\")\n",
    "    \n",
    "#     # Config\n",
    "#     GAP_TOLERANCE = 3500     # 20km gap filling\n",
    "#     INTERP_DISTANCE = 250     # 500m vertex resolution\n",
    "#     PRUNE = 3500\n",
    "    \n",
    "#     if os.path.exists(IN_FILE):\n",
    "#         run_river_skeleton_pipeline(IN_FILE, OUT_FILE, GAP_TOLERANCE, INTERP_DISTANCE, PRUNE)\n",
    "#     else:\n",
    "#         print(f\"File not found: {IN_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db9e538-37f7-4aa8-9d81-9cbd3b882cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Phase 1: Make the rivers flow and make them have the right size.\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     # rivers_path = os.path.join(vector_src_dir, \"wfrp_empire_rivers_line.gpkg\")\n",
    "#     sea_path = os.path.join(vector_src_dir, \"wfrp_empire_sea_poly.gpkg\")\n",
    "#     lakes_path = os.path.join(vector_src_dir, \"wfrp_empire_lakes_poly.gpkg\")\n",
    "#     rivers_path = os.path.join(vector_src_dir, \"wfrp_empire_rivers_line.gpkg\")\n",
    "    \n",
    "#     rivers = gpd.read_file(rivers_path) # The output from skeletonization\n",
    "#     sea = gpd.read_file(sea_path)       # You need this\n",
    "#     lakes = gpd.read_file(lakes_path)   # You need this (Canonical lakes)\n",
    "\n",
    "#     ### Step 1: Orient the rivers in the correct direction.\n",
    "#     analyzer = HydrologyAnalyzer(rivers, sea, lakes)\n",
    "#     oriented_rivers = analyzer.run()\n",
    "#     validate_arrow_directions(oriented_rivers, analyzer.G)\n",
    "#     # plot_flow(oriented_rivers, sea, lakes)\n",
    "\n",
    "#     ### Step 2: Establish river hierarchy\n",
    "#     # 1. Get the Directed Graph from the Phase 1 Analyzer\n",
    "#     flow_calculator = CalculateFlowMagnitude(analyzer.DiG)\n",
    "#     # 2. Calculate Magnitude\n",
    "#     dag_with_flow = flow_calculator.run()\n",
    "#     # 3. Visualize\n",
    "#     # plot_river_hierarchy(dag_with_flow, sea, lakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bd1ff7-e31d-4d6c-9460-30d85ed4b6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     OUT_FILE = os.path.join(vector_src_dir, \"wfrp_empire_major_rivers_net.gpkg\")\n",
    "#     ### Step 3: Work out river widths:\n",
    "#     rivers_with_width = assign_river_widths(\n",
    "#         dag_with_flow, \n",
    "#         min_width=20.0,    # Source streams are 20m wide\n",
    "#         max_width=1000.0,   # The Reik is 800m wide at Altdorf - we make it 1000 for our 250m grid.\n",
    "#         scale_factor=100.0  # Multiplier for the log growth\n",
    "#     )\n",
    "    \n",
    "#     # 2. Restore CRS (Important for buffering!)\n",
    "#     rivers_with_width.set_crs(rivers.crs, inplace=True)\n",
    "    \n",
    "#     # 3. Visualize\n",
    "#     # plot_river_physics(rivers_with_width, sea, lakes)\n",
    "#     save_hydro_network(rivers_with_width, OUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31778896-062f-4025-830d-b5c8e90a34fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TOPOLOGY CONTINUITY CHECK ---\n",
      "Checked 100 junctions.\n",
      "SUCCESS: Water flows continuously from line to line.\n"
     ]
    }
   ],
   "source": [
    "# validate_topology_continuity(rivers_with_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f160d84e-93f7-47b0-8da3-6ba0a1d839de",
   "metadata": {},
   "source": [
    "### Sea set-up\n",
    "\n",
    "Here we prep the sea\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10092cd6-21b7-4ac6-853f-0a187c1fca44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pete/miniconda/envs/gdgtm_dev/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Logging initialized. Writing to: logs/wfrp_phys_geo_20251224_074409.log\n",
      "INFO: Logging initialized. Writing to: logs/wfrp_phys_geo_20251224_074410.log\n",
      "INFO: Logging initialized. Writing to: logs/wfrp_phys_geo_20251224_074410.log\n",
      "INFO: Logging initialized. Writing to: logs/wfrp_phys_geo_20251224_074410.log\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "#  IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "\n",
    "# from world_simulator.terrain_engine import CoordinateEngine\n",
    "# Added necessary utility functions for Step 2\n",
    "from world_simulator.misc_utils import (\n",
    "    vector_to_mask, \n",
    "    generate_fractal_mask, \n",
    "    measure_fractal_dimension,\n",
    "    repair_mask_artifacts\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68e8aeec-3ea3-4fb6-84a4-315f1a2a913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  STEP 1: LOAD CONTEXT DATA\n",
    "# =============================================================================\n",
    "\n",
    "vector_src = \"/home/pete/Documents/wfrp/source_vectors\"\n",
    "raster_src = \"/home/pete/Documents/wfrp/source_rasters\"\n",
    "\n",
    "# A. The Base DEM (Canvas & Starting Heights)\n",
    "base_dem_path = os.path.join(raster_src, \"wfrp_empire_smoothed_topo.tif\") # From your previous step\n",
    "with rasterio.open(base_dem_path) as src:\n",
    "    base_profile = src.profile\n",
    "    base_shape = (src.height, src.width)\n",
    "    base_elevation = src.read(1) # Start with existing terrain\n",
    "\n",
    "# B. Vectors\n",
    "rivers_gdf = gpd.read_file(os.path.join(vector_src, \"wfrp_empire_major_rivers_net.gpkg\"))\n",
    "lakes_gdf = gpd.read_file(\n",
    "    os.path.join(vector_src, \"wfrp_empire_lakes_poly.gpkg\"),\n",
    "    layer=\"wfrp_empire_lakes\")\n",
    "sea_gdf = gpd.read_file(os.path.join(vector_src, \"wfrp_empire_sea_poly.gpkg\")) # Explicit Sea Load\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69b3e802-1a78-4133-8979-3a7c52b830ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Coastline Physics...\n",
      "INFO: Rasterizing 11 vector features into mask...\n",
      "  > Detected Fractal Dimension: 1.224\n",
      "Repairing map boundary artifacts...\n",
      "INFO: Repairing artifacts in 2 zones...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "#  STEP 2: PRE-PROCESS COAST (Measure & Mimic)\n",
    "# =============================================================================\n",
    "# --- A. COASTAL ANALYSIS ---\n",
    "print(\"Analyzing Coastline Physics...\")\n",
    "\n",
    "# 1. Rasterize (Get the shape)\n",
    "raw_sea_mask = vector_to_mask(sea_gdf, base_profile)\n",
    "\n",
    "# Define the artifact zones (Map edges/NoData boundaries)\n",
    "# Format: (West_Col, North_Row, East_Col, South_Row)\n",
    "artifact_boxes = [\n",
    "    (0, 1050, 100, 1300),   # Box 1: West edge artifact\n",
    "    (3500, 0, 7100, 900)    # Box 2: Northern edge artifact\n",
    "]\n",
    "\n",
    "# 2. Measure the \"Native\" Fractal Dimension\n",
    "# We measure the complexity at coarse scales (e.g., 2km to 64km).\n",
    "# This tells us if the coast is \"Scottish\" (High D) or \"Floridian\" (Low D).\n",
    "# We stop at min_scale=16px (~2km) to avoid reading the square pixel artifacts.\n",
    "measured_d = measure_fractal_dimension(\n",
    "    mask=raw_sea_mask,\n",
    "    min_scale=16,   # Ignore details smaller than ~2km\n",
    "    max_scale=512,  # Measure up to ~64km bays\n",
    "    exclusion_boxes=artifact_boxes  # <--- Plugged in here\n",
    ")\n",
    "\n",
    "# print(f\"Measured Fractal Dimension: {measured_d}\")\n",
    "\n",
    "# Safety Clamp: Real coastlines rarely exceed 1.5 or drop below 1.1\n",
    "measured_d = np.clip(measured_d, 1.1, 1.45)\n",
    "print(f\"  > Detected Fractal Dimension: {measured_d:.3f}\")\n",
    "\n",
    "# 3. Synthesize Detail\n",
    "# We use the measured D to generate the sub-pixel details.\n",
    "fractal_sea_mask = generate_fractal_mask(\n",
    "    mask=raw_sea_mask,\n",
    "    fractal_dimension=measured_d,  # <--- The Measurement informs the Synthesis\n",
    "    scale=20.0,\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "# 4. Repair Boundaries\n",
    "# We surgically restore the straight edges in the artifact boxes\n",
    "# to prevent the fractal noise from 'chewing' up the map border.\n",
    "print(\"Repairing map boundary artifacts...\")\n",
    "fractal_sea_mask = repair_mask_artifacts(\n",
    "    fractal_mask=fractal_sea_mask,\n",
    "    reference_mask=raw_sea_mask,      # The clean original\n",
    "    boxes=artifact_boxes,             # <--- Plugged in here again\n",
    "    base_dem=base_elevation,          # Check for valid data\n",
    "    nodata_value=base_profile.get('nodata', -32768)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ce9d759-73aa-4dca-a35c-aa8efd473464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting debug mask to /home/pete/Documents/wfrp/source_rasters/wfrp_coast_mask_test.tif...\n",
      "Debug export complete.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "#  DEBUG EXPORT: FRACTAL SEA MASK\n",
    "# =============================================================================\n",
    "output_mask_path = os.path.join(raster_src, \"wfrp_coast_mask_test.tif\")\n",
    "print(f\"Exporting debug mask to {output_mask_path}...\")\n",
    "\n",
    "# Prepare Profile\n",
    "mask_profile = base_profile.copy()\n",
    "mask_profile.update({\n",
    "    'dtype': 'uint8',\n",
    "    'count': 1,\n",
    "    'compress': 'lzw',\n",
    "    'nodata': 0 \n",
    "})\n",
    "\n",
    "with rasterio.open(output_mask_path, 'w', **mask_profile) as dst:\n",
    "    # Cast Boolean (True/False) to Uint8 (1/0)\n",
    "    # OPTIONAL: Multiply by 255 so it appears as Black/White in QGIS immediately\n",
    "    # without needing to stretch the histogram manually.\n",
    "    debug_data = fractal_sea_mask.astype('uint8') * 255\n",
    "    \n",
    "    dst.write(debug_data, 1)\n",
    "\n",
    "print(\"Debug export complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ff3a76-882d-4c4c-b19c-0774d22101c2",
   "metadata": {},
   "source": [
    "## Topography: \n",
    "\n",
    "The plan is outlined here: https://github.com/pete-jacobsson/enemy_rebuilt/wiki/Topography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61542b5-7177-423a-9ba5-e5e5612a46f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 0. IMPORTS & SETUP\n",
    "# ==============================================================================\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# The Package (To be developed)\n",
    "import pyworldsim.core as core\n",
    "import pyworldsim.generation as gen\n",
    "import pyworldsim.io as io\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865cf7a3-017b-424c-af16-61b96978aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. GLOBAL CONFIGURATION\n",
    "# ==============================================================================\n",
    "# This is the \"Tuning Board\". All magic numbers go here.\n",
    "\n",
    "CONFIG = {\n",
    "    \"paths\": {\n",
    "        \"inputs\": \"/home/pete/Documents/wfrp/physical_layers/inputs\",\n",
    "        \"outputs\": \"/home/pete/Documents/wfrp/physical_layers/outputs\",\n",
    "        \"checkpoints\": \"/home/pete/Documents/wfrp/physical_layers/checkpoints\",\n",
    "        \"template_tif\": \"/home/pete/Documents/wfrp/physical_layers/inputs/physical_template.tif\"\n",
    "        \"vectors\": {\n",
    "            \"bounds\": \"simulation_bounds.gpkg\",\n",
    "            \"sea\": \"sea_polygon.gpkg\",\n",
    "            \"rivers\": \"canonical_rivers.gpkg\",\n",
    "            \"lakes\": \"canonical_lakes.gpkg\",\n",
    "            \"orogeny\": \"orogeny_axes.gpkg\"\n",
    "        }\n",
    "    },\n",
    "    \"resolution\": {\n",
    "        \"sim_pixel_size\": 500.0,  # 500m simulation grid\n",
    "        \"final_pixel_size\": 100.0 # 100m target output\n",
    "    },\n",
    "    \"epochs\": {\n",
    "        \"init\": {\n",
    "            \"base_level\": -50.0,\n",
    "            \"sea_hardness\": 0.05,\n",
    "            \"river_fault_width\": 1500.0 \n",
    "        },\n",
    "        \"orogeny\": {\n",
    "            \"thermal_iters_oe3\": 50,  # Melt the roots\n",
    "            \"thermal_iters_oe2\": 30,  # Round the shield\n",
    "            \"thermal_iters_oe1\": 10,  # Sharpen the peaks\n",
    "            \"talus_angle\": 35.0,\n",
    "            \"subsidence_strength\": 0.8\n",
    "        },\n",
    "        \"glaciation\": {\n",
    "            \"ice_lat_limit\": 1200,    # Y-coordinate or Latitude\n",
    "            \"scour_depth\": 150.0,\n",
    "            \"kettle_density\": 0.02\n",
    "        },\n",
    "        \"breach\": {\n",
    "            \"breach_coords\": (1850, 1400), # Approx coords\n",
    "            \"inertia\": 0.85,               # High inertia = wide curves\n",
    "            \"erosion_width\": 3.0           # 3 pixels (1.5km)\n",
    "        },\n",
    "        \"recent\": {\n",
    "            \"rebound_amp\": 80.0,\n",
    "            \"loess_density\": 1.0           # Multiplier for sediment budget\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(CONFIG[\"paths\"][\"checkpoints\"], exist_ok=True)\n",
    "os.makedirs(CONFIG[\"paths\"][\"outputs\"], exist_ok=True)\n",
    "\n",
    "# Helper for Checkpointing\n",
    "def load_or_run(checkpoint_name, run_func, context):\n",
    "    \"\"\"Checks for checkpoint. If exists, loads it. Else, runs function.\"\"\"\n",
    "    cp_path = Path(CONFIG[\"paths\"][\"checkpoints\"]) / f\"{checkpoint_name}.npz\"\n",
    "    \n",
    "    if cp_path.exists():\n",
    "        print(f\"âœ… Loading Checkpoint: {checkpoint_name}\")\n",
    "        context.load_state(cp_path)\n",
    "    else:\n",
    "        print(f\"ðŸš€ Running Simulation: {checkpoint_name}\")\n",
    "        run_func(context)\n",
    "        print(f\"ðŸ’¾ Saving Checkpoint: {checkpoint_name}\")\n",
    "        context.save_state(cp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c664d7a-eed1-44fa-8192-e307fc3c8b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2. INITIALIZATION (Epoch 0)\n",
    "# ==============================================================================\n",
    "\n",
    "# Initialize the central state object (Holds elevation, masks, layers)\n",
    "# This allocates the 500m/px arrays\n",
    "world = core.WorldState(CONFIG)   ### This will lijkely require the template tif to set the canvas.\n",
    "\n",
    "def run_initialization(ctx):\n",
    "    # 1. Load Vector Inputs (Sea, Bounds, etc.)\n",
    "    ctx.load_vectors()\n",
    "    \n",
    "    # 2. Create Masks (Active Zone, Void, Healer)\n",
    "    gen.ActiveSimulationMask(ctx).apply()\n",
    "    \n",
    "    # 3. Create Base Seabed (-50m)\n",
    "    gen.PrimordialSeabed(level=CONFIG[\"epochs\"][\"init\"][\"base_level\"]).apply(ctx)\n",
    "    \n",
    "    # 4. Burn Lithology (Sea=Sugar, Rivers=Soft)\n",
    "    gen.LithologyManager(ctx).apply_base_lithology(\n",
    "        sea_hardness=CONFIG[\"epochs\"][\"init\"][\"sea_hardness\"]\n",
    "    )\n",
    "    \n",
    "    # 5. Generate Fractal River Weaknesses (Pre-Burn)\n",
    "    gen.RiverNetworkGenerator(ctx).burn_fractures()\n",
    "\n",
    "# Execute\n",
    "load_or_run(\"01_initialization\", run_initialization, world)\n",
    "\n",
    "# Visual Check\n",
    "# plt.imshow(world.elevation, cmap='terrain'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678bf085-4340-4cd8-b92a-08ea9daaf57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 3. OROGENY (Epoch 2: 400MA - 200MA)\n",
    "# ==============================================================================\n",
    "\n",
    "def run_orogeny(ctx):\n",
    "    cfg = CONFIG[\"epochs\"][\"orogeny\"]\n",
    "    \n",
    "    # --- OE3 (Ancient Roots) ---\n",
    "    print(\"  > Processing OE3...\")\n",
    "    gen.VectorOrogeny(ctx, epoch_id=3).apply()\n",
    "    gen.ThermalEroder(ctx).run(iterations=cfg[\"thermal_iters_oe3\"])\n",
    "    \n",
    "    # --- OE2 (The Collision) ---\n",
    "    print(\"  > Processing OE2...\")\n",
    "    gen.VectorOrogeny(ctx, epoch_id=2).apply()\n",
    "    gen.TectonicSubsidence(ctx).apply_talabec_graben()\n",
    "    gen.ThermalEroder(ctx).run(iterations=cfg[\"thermal_iters_oe2\"])\n",
    "    \n",
    "    # --- OE1 (The Great Walls) ---\n",
    "    print(\"  > Processing OE1...\")\n",
    "    gen.VectorOrogeny(ctx, epoch_id=1).apply() # Also applies Metamorphic Halo\n",
    "    \n",
    "    # --- Canonical Lake Basins ---\n",
    "    gen.TectonicSubsidence(ctx).enforce_canonical_lakes()\n",
    "    \n",
    "    # --- Final Pre-Ice Polish ---\n",
    "    gen.ThermalEroder(ctx).run(iterations=cfg[\"thermal_iters_oe1\"])\n",
    "\n",
    "load_or_run(\"02_orogeny\", run_orogeny, world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce78069c-e899-49f5-a627-04b5dc32cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 4. GLACIATION (Epoch 3: 2.5MA)\n",
    "# ==============================================================================\n",
    "\n",
    "def run_glaciation(ctx):\n",
    "    cfg = CONFIG[\"epochs\"][\"glaciation\"]\n",
    "    \n",
    "    # 1. Scour the North (Harvest Sediment)\n",
    "    gen.GlacialScour(ctx).run(\n",
    "        limit_lat=cfg[\"ice_lat_limit\"],\n",
    "        depth=cfg[\"scour_depth\"]\n",
    "    )\n",
    "    # Note: ctx.sediment_budget is now populated\n",
    "    \n",
    "    # 2. Create Kettle Holes (Sour Moors)\n",
    "    gen.EmergentLakeManager(ctx).generate_kettles(density=cfg[\"kettle_density\"])\n",
    "\n",
    "load_or_run(\"03_glaciation\", run_glaciation, world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4036b8bd-5402-4cfc-9099-8edf69491db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 5. CATASTROPHE (Epoch 4: 0.05MA)\n",
    "# ==============================================================================\n",
    "\n",
    "def run_catastrophe(ctx):\n",
    "    cfg = CONFIG[\"epochs\"][\"breach\"]\n",
    "    \n",
    "    # 1. The Altdorf Breach (Inertia Walkers)\n",
    "    # This carves the gorge and dumps the delta\n",
    "    gen.HydraulicBreachEvent(ctx).run(\n",
    "        start_coords=cfg[\"breach_coords\"],\n",
    "        inertia=cfg[\"inertia\"],\n",
    "        width=cfg[\"erosion_width\"]\n",
    "    )\n",
    "\n",
    "load_or_run(\"04_catastrophe\", run_catastrophe, world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93d1527-06b3-4064-afc9-fa07fac1c126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 6. RECENT HISTORY (Epoch 5: Holocene)\n",
    "# ==============================================================================\n",
    "\n",
    "def run_recent(ctx):\n",
    "    cfg = CONFIG[\"epochs\"][\"recent\"]\n",
    "    \n",
    "    # 1. Isostatic Rebound (Tilt the North)\n",
    "    gen.IsostaticRebound(ctx).apply(amplitude=cfg[\"rebound_amp\"])\n",
    "    \n",
    "    # 2. Aeolian Transport (Distribute the Sediment Budget)\n",
    "    gen.AeolianTransport(ctx).distribute_loess(density_mod=cfg[\"loess_density\"])\n",
    "    \n",
    "    # 3. Emergent Oxbows (River Cleanup)\n",
    "    gen.EmergentLakeManager(ctx).generate_oxbows()\n",
    "\n",
    "load_or_run(\"05_recent\", run_recent, world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057c3747-a930-48e3-8dac-ed62817ff5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 7. POST-PROCESSING (Upscale & Texture)\n",
    "# ==============================================================================\n",
    "\n",
    "def run_post_process(ctx):\n",
    "    # 1. Bicubic Upscale (500m -> 100m)\n",
    "    # This returns a NEW high-res object or updates internal state\n",
    "    ctx.upscale_to_target() \n",
    "    \n",
    "    # 2. Apply Phase 4 Noise (Texture)\n",
    "    gen.SurfaceNoiseDetailer(ctx).apply()\n",
    "    \n",
    "    # 3. Classify Soils (Porosity/Nutrients)\n",
    "    gen.SurfaceGeologyClassifier(ctx).classify()\n",
    "    \n",
    "    # 4. Export\n",
    "    io.GeoExporter(ctx).export_all(CONFIG[\"paths\"][\"outputs\"])\n",
    "\n",
    "# Usually we don't checkpoint the huge 100m file, just save the result\n",
    "run_post_process(world)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56363c4-686d-42ed-9ac2-fd15bb57380e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e5e1eb-b2a6-426b-a722-ba01950104aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8a080d-5d6e-4ec5-b20a-ade4e27c8330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290d9a5f-96d5-4f27-acad-a34e4d69849d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(gdgtm_dev)",
   "language": "python",
   "name": "gdgtm_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
